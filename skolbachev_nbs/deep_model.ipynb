{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import time: 13.83356785774231\n",
      "Import time: 0.00010466575622558594\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import local_utils; importlib.reload(local_utils)\n",
    "from local_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y (40479, 17)\n",
      "Shape of train folds (10,)\n",
      "Shape of valid folds (10,)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(DATA_DIR + '/train_v2.csv')\n",
    "test_df = pd.read_csv(DATA_DIR + '/sample_submission_v2.csv')\n",
    "\n",
    "label_map, inv_label_map, Y = process_labels(train_df)\n",
    "print(\"Shape of Y {}\".format(Y.shape))\n",
    "\n",
    "train_folds, valid_folds = stratified_kfold_sampling(Y, n_splits=10, random_state=1000)\n",
    "print(\"Shape of train folds {}\".format(train_folds.shape))\n",
    "print(\"Shape of valid folds {}\".format(valid_folds.shape))\n",
    "\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"deep_model\"\n",
    "\n",
    "fold_inx = 1\n",
    "batch_size = 80\n",
    "all_steps = np.ceil(len(Y)/batch_size)\n",
    "train_steps = np.ceil(len(train_folds[fold_inx])/batch_size)\n",
    "valid_steps = np.ceil(len(valid_folds[fold_inx])/batch_size)\n",
    "test_steps = np.ceil(len(test_df)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cache_images(DATA_DIR+\"/train_images_\"+str(image_size)+\".dat\", train_df['image_name'].values, DATA_DIR + '/train-jpg', True, image_size, image_size, dtype=np.float32)\n",
    "# cache_images(DATA_DIR+\"/test_images_\"+str(image_size)+\".dat\", test_df['image_name'].values, DATA_DIR + '/test-jpg', True, image_size, image_size, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_array = load_array(DATA_DIR+'/train_images_'+str(image_size)+'.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_input = Input(shape=(image_size, image_size, 3))\n",
    "\n",
    "x = BatchNormalization()(x_input)\n",
    "x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "xm = MaxPooling2D()(x)\n",
    "xa = AveragePooling2D()(x)\n",
    "\n",
    "xm = BatchNormalization()(xm)\n",
    "xm = Conv2D(64, (3, 3), padding='same', activation='relu')(xm)\n",
    "xm = Conv2D(64, (3, 3), activation='relu')(xm)\n",
    "xm = MaxPooling2D()(xm)\n",
    "\n",
    "xm = BatchNormalization()(xm)\n",
    "xm = Conv2D(128, (3, 3), padding='same', activation='relu')(xm)\n",
    "xm = Conv2D(128, (3, 3), activation='relu')(xm)\n",
    "xm = MaxPooling2D()(xm)\n",
    "\n",
    "xm = BatchNormalization()(xm)\n",
    "xm = Conv2D(256, (3, 3), padding='same', activation='relu')(xm)\n",
    "xm = Conv2D(256, (3, 3), activation='relu')(xm)\n",
    "xm = GlobalMaxPooling2D()(xm)\n",
    "\n",
    "xa = BatchNormalization()(xa)\n",
    "xa = Conv2D(64, (3, 3), padding='same', activation='relu')(xa)\n",
    "xa = Conv2D(64, (3, 3), activation='relu')(xa)\n",
    "xa = AveragePooling2D()(xa)\n",
    "\n",
    "xa = BatchNormalization()(xa)\n",
    "xa = Conv2D(128, (3, 3), padding='same', activation='relu')(xa)\n",
    "xa = Conv2D(128, (3, 3), activation='relu')(xa)\n",
    "xa = AveragePooling2D()(xa)\n",
    "\n",
    "xa = BatchNormalization()(xa)\n",
    "xa = Conv2D(256, (3, 3), padding='same', activation='relu')(xa)\n",
    "xa = Conv2D(256, (3, 3), activation='relu')(xa)\n",
    "xa = GlobalAveragePooling2D()(xa)\n",
    "\n",
    "x = concatenate([xm, xa])\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "\n",
    "x_output = Dense(17, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getImageDataGenerator():\n",
    "    return ImageDataGenerator(\n",
    "                rotation_range=90,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,)\n",
    "\n",
    "all_gen = getImageDataGenerator()\n",
    "train_gen = getImageDataGenerator()\n",
    "valid_gen = getImageDataGenerator()\n",
    "test_gen = getImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_history = F2History(train_array[valid_folds[fold_inx]], Y[valid_folds[fold_inx]], \n",
    "                       valid_gen, valid_steps, batch_size)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "                DATA_DIR+'/models/'+model_name+'_'+str(fold_inx)+'.h5', \n",
    "                monitor='val_fn_loss', \n",
    "                verbose=1, \n",
    "                save_best_only=True, \n",
    "                save_weights_only=False, \n",
    "                mode='min',\n",
    "                period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del model\n",
    "# model = load_model(DATA_DIR+'/models/'+model_name+'_'+str(fold_inx)+'.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# opt=optimizers.Adam(lr=1e-3, decay=0.5e-5); epochs=20 # 0.33 30\n",
    "# opt=optimizers.Adam(lr=1e-4, decay=0.1e-5); epochs=10 # 0.66 15\n",
    "# opt=optimizers.Adam(lr=1e-5, decay=0.1e-6); epochs=5\n",
    "# opt=optimizers.Adam(lr=0.5e-5, decay=0.1e-6); epochs=5 #?\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fn_loss, f2, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    generator=train_gen.flow(train_array[train_folds[fold_inx]], Y[train_folds[fold_inx]], batch_size=batch_size),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_gen.flow(train_array[valid_folds[fold_inx]], Y[valid_folds[fold_inx]], batch_size=batch_size, shuffle=False),\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=[model_checkpoint, f2_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f2_history.f2_02_scores)\n",
    "plt.plot(f2_history.f2_02_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f2_history.f2_05_scores)\n",
    "plt.plot(f2_history.f2_05_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(DATA_DIR+'/models/'+model_name+'_'+str(fold_inx)+'.h5', compile=False)\n",
    "\n",
    "opt=optimizers.Adam(lr=1e-5, decay=0.1e-6); epochs=5;\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fn_loss, f2, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 28s - loss: 0.0900 - fn_loss: 0.1114 - f2: 0.8975 - acc: 0.9657    \n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 26s - loss: 0.0917 - fn_loss: 0.1137 - f2: 0.8937 - acc: 0.9648    \n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 27s - loss: 0.0909 - fn_loss: 0.1119 - f2: 0.8984 - acc: 0.9658    \n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 27s - loss: 0.0913 - fn_loss: 0.1129 - f2: 0.8958 - acc: 0.9650    \n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 27s - loss: 0.0910 - fn_loss: 0.1130 - f2: 0.8942 - acc: 0.9646    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0472d3a128>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(\n",
    "#     generator=valid_gen.flow(train_array[valid_folds[fold_inx]], Y[valid_folds[fold_inx]], batch_size=batch_size),\n",
    "#     steps_per_epoch=valid_steps,\n",
    "#     epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36430/36430 [==============================] - 79s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.078270561357071985,\n",
       " 0.097470962605304912,\n",
       " 0.90946767452448951,\n",
       " 0.96901553416036168]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.evaluate(train_array[train_folds[fold_inx]], Y[train_folds[fold_inx]], batch_size=batch_size)\n",
    "# model.save(DATA_DIR+'/models/'+model_name+'_'+str(fold_inx)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.087895764791029243,\n",
       " 0.10899029899950703,\n",
       " 0.90044955881883126,\n",
       " 0.96645506030690609]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(\n",
    "    valid_gen.flow(train_array[valid_folds[fold_inx]], Y[valid_folds[fold_inx]], batch_size=batch_size, shuffle=False),\n",
    "    steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.evaluate_generator(\n",
    "#     all_gen.flow(train_array, Y, batch_size=batch_size, shuffle=False),\n",
    "#     steps=all_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40479/40479 [==============================] - 85s    \n",
      "506/506 [==============================] - 85s    \n",
      "506/506 [==============================] - 86s    \n",
      "506/506 [==============================] - 86s    \n",
      "506/506 [==============================] - 85s    \n",
      "506/506 [==============================] - 86s    \n",
      "506/506 [==============================] - 85s    \n",
      "506/506 [==============================] - 85s    \n",
      "506/506 [==============================] - 86s    \n",
      "506/506 [==============================] - 86s    \n",
      "506/506 [==============================] - 86s    \n"
     ]
    }
   ],
   "source": [
    "p_X_no_tta, p_X_tta_list = predictTTA(model, train_array, all_gen, all_steps, batch_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X_tta = sum(p_X_tta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 11.0\n",
    "p_X = (c*p_X_no_tta+p_X_tta)/(c+10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Score 0.9273439251311457 with 0.297\n",
      "4: Score 0.927050590753084 with 0.203\n",
      "1: Score 0.9270483290812743 with 0.209\n",
      "7: Score 0.9270454691607924 with 0.219\n",
      "2: Score 0.9271454898947575 with 0.169\n",
      "5: Score 0.9274536317458528 with 0.28650000000000003\n",
      "6: Score 0.927818510755448 with 0.096\n",
      "3: Score 0.9271710534536511 with 0.1695\n",
      "8: Score 0.9271329448524258 with 0.196\n",
      "11: Score 0.9270790462605422 with 0.2135\n",
      "14: Score 0.9270931147542137 with 0.11950000000000001\n",
      "9: Score 0.9272809471159886 with 0.2335\n",
      "12: Score 0.9273494192811049 with 0.248\n",
      "13: Score 0.9273578145478347 with 0.2715\n",
      "10: Score 0.9272197151562164 with 0.2315\n",
      "15: Score 0.9270642106737617 with 0.21\n",
      "16: Score 0.9272157781065562 with 0.2115\n",
      "F2 Valid Score: 0.9302978596910565\n",
      "F2 All Score: 0.9323065168506792\n",
      "29.770496129989624\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "thres = find_best_thresholds(Y[valid_folds[fold_inx]], p_X[valid_folds[fold_inx]])\n",
    "print(\"F2 Valid Score: {}\".format(f2_score(Y[valid_folds[fold_inx]], p_X[valid_folds[fold_inx]], thres)))\n",
    "print(\"F2 All Score: {}\".format(f2_score(Y, p_X, thres)))\n",
    "\n",
    "save_array(DATA_DIR+'/models/'+model_name+'_thres_'+str(fold_inx)+'.dat', thres)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930297859691\n",
      "0.932306516851\n"
     ]
    }
   ],
   "source": [
    "# thres = load_array(DATA_DIR+'/models/'+model_name+'_thres_'+str(fold_inx)+'.dat')\n",
    "print(f2_score(Y[valid_folds[fold_inx]], p_X[valid_folds[fold_inx]], thres))\n",
    "print(f2_score(Y, p_X, thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# max_f2 = 0.0\n",
    "# best_c = 0\n",
    "# for c in range(1, 15):\n",
    "#     print(c)\n",
    "#     p_X = (c*p_X_no_tta+p_X_tta)/(10.0 + c)\n",
    "#     thres = find_best_thresholds(Y[valid_folds[fold_inx]], p_X[valid_folds[fold_inx]])\n",
    "#     current_f2 = f2_score(Y[valid_folds[fold_inx]], p_X[valid_folds[fold_inx]], thres)\n",
    "    \n",
    "#     print(\"F2 Valid Score: {}\".format(current_f2))\n",
    "#     print(\"F2 All Score: {}\".format(f2_score(Y, p_X, thres)))\n",
    "    \n",
    "#     if current_f2>max_f2:\n",
    "#         max_f2 = current_f2\n",
    "#         best_c = c\n",
    "\n",
    "# print(max_f2)\n",
    "# print(best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.89811897277832\n"
     ]
    }
   ],
   "source": [
    "del train_array\n",
    "del p_X\n",
    "del p_X_no_tta\n",
    "del p_X_tta\n",
    "del p_X_tta_list\n",
    "del model\n",
    "del thres\n",
    "\n",
    "start = time()\n",
    "model = load_model(DATA_DIR+'/models/'+model_name+'_'+str(fold_inx)+'.h5', compile=False)\n",
    "thres = load_array(DATA_DIR+'/models/'+model_name+'_thres_'+str(fold_inx)+'.dat')\n",
    "\n",
    "test_array = load_array(DATA_DIR+'/test_images_'+str(image_size)+'.dat')\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61191/61191 [==============================] - 129s   \n",
      "765/765 [==============================] - 129s   \n",
      "765/765 [==============================] - 129s   \n",
      "765/765 [==============================] - 129s   \n",
      "765/765 [==============================] - 129s   \n",
      "765/765 [==============================] - 130s   \n",
      "765/765 [==============================] - 130s   \n",
      "765/765 [==============================] - 130s   \n",
      "765/765 [==============================] - 130s   \n",
      "765/765 [==============================] - 130s   \n",
      "765/765 [==============================] - 129s   \n"
     ]
    }
   ],
   "source": [
    "p_X_test_no_tta, p_X_test_tta_list = predictTTA(model, test_array, test_gen, test_steps, batch_size, 10)\n",
    "p_X_test_tta = sum(p_X_test_tta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 20.0\n",
    "p_X_test = (c*p_X_test_no_tta+p_X_test_tta)/(c+10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.340683698654175\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>partly_cloudy primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>clear cultivation primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>cloudy partly_cloudy primary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                          tags\n",
       "0     test_0                 clear primary\n",
       "1     test_1                 clear primary\n",
       "2     test_2         partly_cloudy primary\n",
       "3     test_3     clear cultivation primary\n",
       "4     test_4  cloudy partly_cloudy primary"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "for inx in np.arange(len(test_df)):\n",
    "    test_df['tags'][inx] = ' '.join([inv_label_map[i] for i, b in enumerate(p_X_test[inx] >= thres) if b])\n",
    "print(time() - start)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/results/deep_model_submission.csv' target='_blank'>data/results/deep_model_submission.csv</a><br>"
      ],
      "text/plain": [
       "/src/DL/planet_understanding_the_amazon_from_space/data/results/deep_model_submission.csv"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_array(DATA_DIR+'/results/'+model_name+'_test.dat', p_X_test)\n",
    "test_df.to_csv(DATA_DIR+'/results/'+model_name+'_submission.csv', index=False)\n",
    "FileLink('data/results/'+model_name+'_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.618086404403051\n"
     ]
    }
   ],
   "source": [
    "print((time() - global_start)/60/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
