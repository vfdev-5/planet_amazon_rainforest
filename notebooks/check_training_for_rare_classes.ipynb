{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    if __file__: exit\n",
    "except NameError:\n",
    "    __file__ = '.'\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "    \n",
    "# Project\n",
    "project_common_path = os.path.dirname(__file__)\n",
    "project_common_path = os.path.abspath(os.path.join(project_common_path, '..', 'common'))\n",
    "if not project_common_path in sys.path:\n",
    "    sys.path.append(project_common_path)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data_utils import get_id_type_list_for_class, OUTPUT_PATH, GENERATED_DATA, to_set\n",
    "from training_utils import classification_train as train, classification_validate as validate\n",
    "from training_utils import exp_decay, step_decay, get_basic_imgaug_seq\n",
    "\n",
    "from models.mini_vgg_multiclassification import get_mini_vgg\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from data_utils import to_set, equalized_data_classes, unique_tags, train_jpg_ids, TRAIN_ENC_CL_CSV\n",
    "from data_utils import load_pretrained_model, get_label\n",
    "from data_utils import DataCache\n",
    "\n",
    "from xy_providers import tif_image_label_provider\n",
    "from models.keras_metrics import binary_crossentropy_with_false_negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 2017\n",
    "np.random.seed(seed)\n",
    "\n",
    "cache = DataCache(10000)  # !!! CHECK BEFORE LOAD TO FLOYD\n",
    "\n",
    "class_index = 0\n",
    "trainval_id_type_list = get_id_type_list_for_class(class_index, 'Train_tif')\n",
    "\n",
    "from glob import glob\n",
    "from data_utils import GENERATED_DATA, to_set, get_label\n",
    "\n",
    "gen_train_files = glob(os.path.join(GENERATED_DATA, \"train\", \"tif\", \"*.tif\"))\n",
    "gen_train_ids = [s[len(os.path.join(GENERATED_DATA, \"train\", \"tif\"))+1+len('gen_train_'):-4] for s in gen_train_files]\n",
    "gen_id_type_list = [(image_id, \"Generated_Train_tif\") for image_id in gen_train_ids]\n",
    "class_index_gen_train_ids = [id_type for id_type in gen_train_ids if np.sum(get_label(*gen_id_type_list[0], class_index=class_index)) > 0]\n",
    "class_index_gen_id_type_list = [(image_id, \"Generated_Train_tif\") for image_id in class_index_gen_train_ids]\n",
    "trainval_id_type_list = trainval_id_type_list + class_index_gen_id_type_list\n",
    "\n",
    "class_indices = list(equalized_data_classes.keys())\n",
    "class_indices.remove(class_index)\n",
    "\n",
    "n_other_samples = int(len(trainval_id_type_list) * 1.0 / len(class_indices) / len(equalized_data_classes[class_index]))\n",
    "\n",
    "for index in class_indices:\n",
    "    id_type_list = np.array(get_id_type_list_for_class(index, 'Train_tif'))\n",
    "    id_type_list = list(to_set(id_type_list) - to_set(trainval_id_type_list))\n",
    "    np.random.shuffle(id_type_list)\n",
    "    trainval_id_type_list.extend(id_type_list[:n_other_samples])\n",
    "\n",
    "\n",
    "params = {\n",
    "    'seed': seed,\n",
    "\n",
    "    'xy_provider': tif_image_label_provider,\n",
    "\n",
    "    'network': get_mini_vgg,\n",
    "    'optimizer': 'adadelta',\n",
    "    'loss': 'binary_crossentropy', # mae_with_false_negatives,\n",
    "    'nb_epochs': 25,    # !!! CHECK BEFORE LOAD TO FLOYD\n",
    "    'batch_size': 16,  # !!! CHECK BEFORE LOAD TO FLOYD\n",
    "\n",
    "    'normalize_data': False,\n",
    "    'normalization': '',\n",
    "\n",
    "    'image_size': (224, 224),\n",
    "\n",
    "    'class_index': class_index,\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    'lr_kwargs': {\n",
    "        'lr': 0.1,\n",
    "        'a': 0.95,\n",
    "        'init_epoch': 0\n",
    "    },\n",
    "    'lr_decay_f': exp_decay,\n",
    "\n",
    "    'train_seq': get_basic_imgaug_seq,\n",
    "\n",
    "    # Reduce learning rate on plateau\n",
    "    'on_plateau': True,\n",
    "    'on_plateau_kwargs': {\n",
    "        'monitor': 'val_loss',\n",
    "        'factor': 0.1,\n",
    "        'patience': 3,\n",
    "        'verbose': 1\n",
    "    },\n",
    "\n",
    "    'cache': cache,\n",
    "\n",
    "#     'class_index': 0,\n",
    "    'pretrained_model': 'load_best',\n",
    "#     'pretrained_model': os.path.join(GENERATED_DATA, \"weights\", \"\"),\n",
    "\n",
    "    'output_path': OUTPUT_PATH,\n",
    "}\n",
    "\n",
    "params['save_prefix_template'] = '{cnn_name}_tif_class=%i_fold={fold_index}_seed=%i' % (class_index, params['seed'])\n",
    "params['input_shape'] = params['image_size'] + (7,)\n",
    "params['n_classes'] = len(equalized_data_classes[class_index]) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_utils import get_label\n",
    "labels = np.zeros((len(trainval_id_type_list), len(equalized_data_classes[class_index])+1), dtype=np.uint8)\n",
    "doubles = []\n",
    "for i, it in enumerate(trainval_id_type_list):\n",
    "    labels[i, :-1] = get_label(*it, class_index=class_index)\n",
    "    if np.sum(labels[i, :-1]) < 1:\n",
    "        labels[i, -1] = 1\n",
    "    if np.sum(labels[i, :-1]) > 1:\n",
    "        doubles.append(labels[i, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1356, 1328,  392,  400, 1360,  836, 2784], dtype=uint64), 8356)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = np.sum(labels, axis=0)\n",
    "stats, len(trainval_id_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ---- Validation fold index:  1 / 5\n",
      "2017-07-17 21:51:20.985624 6900 1725\n",
      "\n",
      " 2017-07-17 21:51:21.431645 - Loaded mini-VGG model ...\n",
      "Load best loss weights:  /Users/vfomin/Documents/ML/Kaggle/PlanetAmazonRainForest/common/../output/weights/mini-VGG_tif_class=0_fold=0_seed=2017_00_val_loss=0.4504_val_precision=0.6705_val_recall=0.1857_val_f2=0.1873.h5 0.4504\n",
      "\n",
      " 2017-07-17 21:51:23.135114 - Start training ...\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "val_fold_index = 0\n",
    "val_fold_indices = []  # !!! CHECK BEFORE LOAD TO FLOYD\n",
    "hists = []\n",
    "\n",
    "kf = KFold(n_splits=n_folds)\n",
    "trainval_id_type_list = np.array(trainval_id_type_list)\n",
    "for train_index, test_index in kf.split(trainval_id_type_list):\n",
    "    train_id_type_list, val_id_type_list = trainval_id_type_list[train_index], trainval_id_type_list[test_index]\n",
    "\n",
    "    if len(val_fold_indices) > 0:\n",
    "        if val_fold_index not in val_fold_indices:\n",
    "            val_fold_index += 1\n",
    "            continue\n",
    "                \n",
    "    val_fold_index += 1\n",
    "    print(\"\\n\\n ---- Validation fold index: \", val_fold_index, \"/\", n_folds)\n",
    "\n",
    "    print(datetime.now(), len(train_id_type_list), len(val_id_type_list))\n",
    "    assert len(to_set(train_id_type_list) & to_set(val_id_type_list)) == 0, \"WTF\"\n",
    "\n",
    "    weights = None if 'pretrained_model' in params else None\n",
    "    cnn = params['network'](lr=params['lr_kwargs']['lr'], weights=weights, **params)\n",
    "    params['save_prefix'] = params['save_prefix_template'].format(cnn_name=cnn.name, fold_index=val_fold_index - 1)\n",
    "    print(\"\\n {} - Loaded {} model ...\".format(datetime.now(), cnn.name))\n",
    "\n",
    "    if 'pretrained_model' in params:\n",
    "        load_pretrained_model(cnn, **params)\n",
    "        \n",
    "    print(\"\\n {} - Start training ...\".format(datetime.now()))\n",
    "\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, Callback\n",
    "import keras.backend as K\n",
    "from keras import __version__ as keras_version\n",
    "\n",
    "\n",
    "# Local repos:\n",
    "local_repos_path = os.path.abspath(os.path.dirname(__file__))\n",
    "\n",
    "keras_contrib_path = os.path.join(local_repos_path, \"KerasContrib\", \"keras_contrib\")\n",
    "if keras_contrib_path not in sys.path:\n",
    "    sys.path.append(keras_contrib_path)\n",
    "\n",
    "imgaug_contrib_path = os.path.join(local_repos_path, \"imgaug\", \"imgaug\")\n",
    "if imgaug_contrib_path not in sys.path:\n",
    "    sys.path.append(imgaug_contrib_path)\n",
    "\n",
    "\n",
    "from preprocessing.image.generators import ImageDataGenerator\n",
    "from imgaug.imgaug import augmenters as iaa\n",
    "\n",
    "from data_utils import GENERATED_DATA, OUTPUT_PATH, unique_tags\n",
    "from metrics import score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from postproc import pred_threshold\n",
    "\n",
    "from training_utils import write_info, get_train_imgaug_seq, get_val_imgaug_seq, get_gen_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert 'batch_size' in params, \"Need batch_size\"\n",
    "assert 'save_prefix' in params, \"Need save_prefix\"\n",
    "assert 'nb_epochs' in params, \"Need nb_epochs\"\n",
    "assert 'seed' in params, \"Need seed\"\n",
    "assert 'normalize_data' in params, \"Need normalize_data\"\n",
    "\n",
    "model = cnn\n",
    "\n",
    "samples_per_epoch = len(train_id_type_list) if 'samples_per_epoch' not in params else params['samples_per_epoch']\n",
    "nb_val_samples = len(val_id_type_list) if 'nb_val_samples' not in params else params['nb_val_samples']\n",
    "lr_decay_f = None if 'lr_decay_f' not in params else params['lr_decay_f']\n",
    "\n",
    "save_prefix = params['save_prefix']\n",
    "batch_size = params['batch_size']\n",
    "nb_epochs = params['nb_epochs']\n",
    "seed = params['seed']\n",
    "normalize_data = params['normalize_data']\n",
    "if normalize_data:\n",
    "    assert 'normalization' in params, \"Need normalization\"\n",
    "    normalization = params['normalization']\n",
    "else:\n",
    "    normalization = None\n",
    "\n",
    "samples_per_epoch = (samples_per_epoch // batch_size + 1) * batch_size\n",
    "nb_val_samples = (nb_val_samples // batch_size + 1) * batch_size\n",
    "\n",
    "output_path = params['output_path'] if 'output_path' in params else GENERATED_DATA\n",
    "weights_path = os.path.join(output_path, \"weights\")\n",
    "if not os.path.exists(weights_path):\n",
    "    os.mkdir(weights_path)\n",
    "\n",
    "weights_filename = os.path.join(weights_path,\n",
    "                                save_prefix + \"_{epoch:02d}_val_loss={val_loss:.4f}\")\n",
    "\n",
    "metrics_names = list(model.metrics_names)\n",
    "metrics_names.remove('loss')\n",
    "for mname in metrics_names:\n",
    "    weights_filename += \"_val_%s={val_%s:.4f}\" % (mname, mname)\n",
    "weights_filename += \".h5\"\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(weights_filename, monitor='val_loss',\n",
    "                                   save_best_only=False, save_weights_only=True)\n",
    "now = datetime.now()\n",
    "info_filename = os.path.join(weights_path,\n",
    "                             'training_%s_%s.info' % (save_prefix, str(now.strftime(\"%Y-%m-%d-%H-%M\"))))\n",
    "\n",
    "write_info(info_filename, **params)\n",
    "\n",
    "csv_logger = CSVLogger(os.path.join(weights_path,\n",
    "                                    'training_%s_%s.log' % (save_prefix, str(now.strftime(\"%Y-%m-%d-%H-%M\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Training parameters: 16, 25, 6912, 1728\n",
      "\n",
      "-- Fit model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks = [model_checkpoint, csv_logger,]\n",
    "if lr_decay_f is not None:\n",
    "    assert 'lr_kwargs' in params and \\\n",
    "           isinstance(params['lr_kwargs'], dict), \"Need lr_kwargs\"\n",
    "    _lr_decay_f = lambda e: lr_decay_f(epoch=e, **params['lr_kwargs'])\n",
    "    lrate = LearningRateScheduler(_lr_decay_f)\n",
    "    callbacks.append(lrate)\n",
    "if 'on_plateau' in params and params['on_plateau']:\n",
    "    if 'on_plateau_kwargs' in params and \\\n",
    "            isinstance(params['lr_kwargs'], dict):\n",
    "        onplateau = ReduceLROnPlateau(**params['on_plateau_kwargs'])\n",
    "    else:\n",
    "        onplateau = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1)\n",
    "    callbacks.append(onplateau)\n",
    "\n",
    "print(\"\\n-- Training parameters: %i, %i, %i, %i\" % (batch_size, nb_epochs, samples_per_epoch, nb_val_samples))\n",
    "print(\"\\n-- Fit model\")\n",
    "\n",
    "class_weight = params.get('class_weight')\n",
    "verbose = 1 if 'verbose' not in params else params['verbose']\n",
    "\n",
    "if 'train_seq' in params:\n",
    "    assert callable(params['train_seq']), \"params['train_seq'] should be callable\"\n",
    "train_seq = get_train_imgaug_seq(seed) if 'train_seq' not in params else params['train_seq'](seed)\n",
    "\n",
    "if 'val_seq' in params:\n",
    "    assert callable(params['val_seq']), \"params['val_seq'] should be callable\"\n",
    "val_seq = get_val_imgaug_seq(seed) if 'val_seq' not in params else params['val_seq'](seed)\n",
    "\n",
    "train_gen, train_flow = get_gen_flow(id_type_list=train_id_type_list,\n",
    "                                     imgaug_seq=train_seq,\n",
    "                                     **params)\n",
    "\n",
    "if normalize_data and normalization == '':\n",
    "    params['normalization'] = 'from_save_prefix'\n",
    "\n",
    "val_gen, val_flow = get_gen_flow(id_type_list=val_id_type_list,\n",
    "                                 imgaug_seq=val_seq,\n",
    "                                 **params)\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for x, y in train_flow:\n",
    "#     model.train_on_batch(x, y)\n",
    "#     print(\"-- %i\" % counter)\n",
    "#     counter+=1\n",
    "#     if counter == 10:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1]\n",
      " [0 0 0 1 0 0 1]\n",
      " [0 1 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0]\n",
      " [1 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]]\n",
      "[[0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]]\n",
      "0.385416666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_on_batch(x)\n",
    "print((y_pred > 0.35).astype(np.uint8))\n",
    "print(y)\n",
    "print(score(y, (y_pred > 0.35).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x2, y2 in val_flow:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metrics import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0]]\n",
      "0.520833333333\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_on_batch(x2)\n",
    "print((y_pred > 0.1).astype(np.uint8))\n",
    "print(y2)\n",
    "print(score(y2, (y_pred > 0.1).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
